{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRUBidirectionnelle_ProteinStructure.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Check Device for GPU\n",
        "'''\n",
        "Si Pytorch trouve \n",
        "GPU disponible, les données et le modèle seront poussés vers le GPU pour l'entrainement. \n",
        "'''\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "8dl7xAGW5K7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports nécéssaires"
      ],
      "metadata": {
        "id": "YqQAI77wzrdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.eager.monitoring import MonitoredTimer\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from time import time\n",
        "from torch import nn,optim\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import re\n",
        "import torch\n",
        "import torchvision\n",
        "import math\n",
        "\n",
        "#Information Utiles\n",
        "''' acide aminé train 18105 et target 111\n",
        "\n",
        "    acide aminé test 3520 et target 17\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "#Maximum length est de  498"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UwfNU3oerOIQ",
        "outputId": "16937736-e4fc-4802-e02c-be8a9d912034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' acide aminé train 18105 et target 111\\n\\nacide aminé test 3520 et target 17\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cette cellule :\n",
        "**********\n",
        "***lirefile*** vas lire le fichier en brut et renvoyé une list of list des proteine chaque proteine dans une list, et pour labels  également une list of list des targets.\n",
        "*******\n",
        "je définit les dictionnaires avec chaque acide et targets son numéro (ac_am_dict , targ_dict )\n",
        "*******\n",
        "***TransformAcideAminTRAIN*** :\n",
        "retourne un dataframe avec les target et acide aminé tous de meme taille et avec des numéro entier pour facilité leur transformation en one hot\n",
        "Par exemple : [3, 11, 9, 8, 9, 5, 4, 9, 17, 8, 13, 12, 3, 3,...] "
      ],
      "metadata": {
        "id": "vj3LIKrbzuzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data2\n",
        "def lirefile(file_name):\n",
        "    data=[]\n",
        "    target=[]\n",
        "    \n",
        "    \n",
        "    with open(file_name, 'r', encoding='utf-8') as raw_file:\n",
        "        for i,line in enumerate(raw_file):\n",
        "            line_content = re.sub('[\\n\\t]', '', line)\n",
        "            if line_content not in [\"end\",'<>']:\n",
        "                d=line_content.split(\" \")\n",
        "                data[-1].append(d[0])\n",
        "                target[-1].append(d[1])\n",
        "            elif line_content in ['<>']:\n",
        "                data.append([])\n",
        "                target.append([])\n",
        "    return data , target\n",
        "\n",
        "dataTrain , labelTrain =lirefile(\"/content/drive/MyDrive/AvignonM1_ApplicationIA/DATA/protein-secondary-structure (1).train\")\n",
        "dataTest , labelTest =lirefile(\"/content/drive/MyDrive/AvignonM1_ApplicationIA/DATA/protein-secondary-structure.test\")\n",
        "print(dataTest)\n",
        "print(labelTest)\n",
        "\n",
        "ac_am_dict = {\n",
        "    \n",
        "    \"A\": 0,\n",
        "    \"C\": 1,\n",
        "    \"D\": 2,\n",
        "    \"E\": 3,\n",
        "    \"F\": 4,\n",
        "    \"G\": 5,\n",
        "    \"H\": 6,\n",
        "    \"I\": 7,\n",
        "    \"K\": 8,\n",
        "    \"L\": 9,\n",
        "    \"M\": 10,\n",
        "    \"N\": 11,\n",
        "    \"P\": 12,\n",
        "    \"Q\": 13,\n",
        "    \"R\": 14,\n",
        "    \"S\": 15,\n",
        "    \"T\": 16,\n",
        "    \"V\": 17,\n",
        "    \"W\": 18,\n",
        "    \"Y\": 19,\n",
        "    \"0\": 20,\n",
        "    \n",
        "    \n",
        "     \n",
        "    \n",
        "}\n",
        "\n",
        "targ_dict = {\n",
        "    \n",
        "    \"_\": 0,\n",
        "    \"e\": 1,\n",
        "    \"h\": 2,\n",
        "    \"0\": 3,\n",
        "    \n",
        "    \n",
        "    \n",
        "}\n",
        " \n",
        "\n",
        " \n",
        "def TransformAcideAmin(DataFrame):\n",
        "     #Normalisation pour avoir des protein de meme taill \n",
        "     for i,row in DataFrame.iterrows():\n",
        "        if 498 != len(row['acideAmine']):\n",
        "            more= 498 - len(row['acideAmine'])\n",
        "            row['acideAmine'].extend(['0']*more)\n",
        "        #remplissage avec padding des 0\n",
        "        if 498 != len(row['label']):\n",
        "            more=498 - len(row['label'])\n",
        "            row['label'].extend(['0']*more)\n",
        "        #une fois le row transformé je l'ajout au data frame\n",
        "        DataFrame.at[i,'acideAmine'] = row['acideAmine']\n",
        "        DataFrame.at[i,'label'] = row['label']\n",
        "\n",
        "    #a cette étapes j'ai un dataframes de proteines et target avec le padding mais encore avec les données brute  \n",
        "     for i,row in DataFrame.iterrows():\n",
        "        a = []\n",
        "        #chaq acide  sera remplacé par son numéro du dictionnaire\n",
        "        for acide in row['acideAmine']:    \n",
        "            a.append(ac_am_dict[acide])\n",
        "            \n",
        "\n",
        "\n",
        "        DataFrame.at[i,\"acideAmine\"] = a\n",
        "      \n",
        "        target = []\n",
        "        #chaq target  sera remplacé par son numéro du dictionnaire\n",
        "        for trgt in row['label']:\n",
        "            target.append(targ_dict[trgt])\n",
        "            \n",
        "        DataFrame.at[i,\"label\"] = target\n",
        "        \n",
        "\n",
        "     return DataFrame\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Je crée le dataframe qui sera l'entrée de TransformAcideAmin\n",
        "train = pd.DataFrame({'acideAmine':dataTrain,'label':labelTrain})\n",
        "test = pd.DataFrame({'acideAmine':dataTest,'label':labelTest})\n",
        "\n",
        "\n",
        "FinaleTRAIN = TransformAcideAmin(train)\n",
        "FinaleTest = TransformAcideAmin(test)\n",
        "print(FinaleTest)\n",
        "#ici à la sortie j'aurai des dataframe avec les données de meme taille et leurs numéros adéquat\n",
        "\n",
        "#je transforme mes données en one hot\n",
        "x_train = np.array(FinaleTRAIN['acideAmine'].to_list())\n",
        "y_train = np.array(FinaleTRAIN['label'].to_list())\n",
        "x_test = np.array(FinaleTest['acideAmine'].to_list())\n",
        "y_test = np.array(FinaleTest['label'].to_list())\n",
        "\n",
        "\n",
        "#transformé en torch\n",
        "x_train = torch.from_numpy(tf.one_hot(x_train, depth =21).numpy()).to(device)\n",
        "x_test = torch.from_numpy(tf.one_hot(x_test, depth =21).numpy()).to(device)\n",
        "y_train = torch.from_numpy(tf.one_hot(y_train, depth =4).numpy()).to(device)\n",
        "y_test = torch.from_numpy(tf.one_hot(y_test, depth =4).numpy()).to(device)\n",
        "#par exemple : [0., 0., 0.,  ..., 0., 0., 0.]\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test)\n",
        "print(x_test)\n",
        "\n",
        "\n",
        "#cette fonction pour cree un dataset je l'utilise pas\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    def __init__(self,X,Y):\n",
        "        \n",
        "        self.x_data = X\n",
        "       \n",
        "            \n",
        "        self.y_data =Y\n",
        "        \n",
        "        \n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        return len(self.y_data)\n",
        "\n",
        "\n",
        "        \n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        #\n",
        "        return self.x_data[index],self.y_data[index]\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "# Je Cree TensorDataSet avec la fonction prédef\n",
        "traindata = TensorDataset(x_train,y_train)\n",
        "testdata = TensorDataset(x_test,y_test)\n",
        "#tensor([[[0., 0., 0.,  ..., 0., 0., 0.]\n",
        "#J'appele le dataloader pour un lot de données de 16 pour les deux train et test\n",
        "loadtrain = DataLoader(traindata, batch_size =16 )\n",
        "loadtest = DataLoader(testdata, batch_size = 16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTLO-JrXrHyn",
        "outputId": "bfcf7dae-a184-47d3-c02d-9ae591fb61ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['E', 'N', 'L', 'K', 'L', 'G', 'F', 'L', 'V', 'K', 'Q', 'P', 'E', 'E', 'P', 'W', 'F', 'Q', 'T', 'E', 'W', 'K', 'F', 'A', 'D', 'K', 'A', 'G', 'K', 'D', 'L', 'G', 'F', 'E', 'V', 'I', 'K', 'I', 'A', 'V', 'P', 'D', 'G', 'E', 'K', 'T', 'L', 'N', 'A', 'I', 'D', 'S', 'L', 'A', 'A', 'S', 'G', 'A', 'K', 'G', 'F', 'V', 'I', 'C', 'T', 'P', 'D', 'P', 'K', 'L', 'G', 'S', 'A', 'I', 'V', 'A', 'K', 'A', 'R', 'G', 'Y', 'D', 'M', 'K', 'V', 'I', 'A', 'V', 'D', 'D', 'Q', 'F', 'V', 'N', 'A', 'K', 'G', 'K', 'P', 'M', 'D', 'T', 'V', 'P', 'L', 'V', 'M', 'M', 'A', 'A', 'T', 'K', 'I', 'G', 'E', 'R', 'Q', 'G', 'Q', 'E', 'L', 'Y', 'K', 'E', 'M', 'Q', 'K', 'R', 'G', 'W', 'D', 'V', 'K', 'E', 'S', 'A', 'V', 'M', 'A', 'I', 'T', 'A', 'N', 'E', 'L', 'D', 'T', 'A', 'R', 'R', 'R', 'T', 'T', 'G', 'S', 'M', 'D', 'A', 'L', 'K', 'A', 'A', 'G', 'F', 'P', 'E', 'K', 'Q', 'I', 'Y', 'Q', 'V', 'P', 'T', 'K', 'S', 'N', 'D', 'I', 'P', 'G', 'A', 'F', 'D', 'A', 'A', 'N', 'S', 'M', 'L', 'V', 'Q', 'H', 'P', 'E', 'V', 'K', 'H', 'W', 'L', 'I', 'V', 'G', 'M', 'N', 'D', 'S', 'T', 'V', 'L', 'G', 'G', 'V', 'R', 'A', 'T', 'E', 'G', 'Q', 'G', 'F', 'K', 'A', 'A', 'D', 'I', 'I', 'G', 'I', 'G', 'I', 'N', 'G', 'V', 'D', 'A', 'V', 'S', 'E', 'L', 'S', 'K', 'A', 'Q', 'A', 'T', 'G', 'F', 'Y', 'G', 'S', 'L', 'L', 'P', 'S', 'P', 'D', 'V', 'H', 'G', 'Y', 'K', 'S', 'S', 'E', 'M', 'L', 'Y', 'N', 'W', 'V', 'A', 'K', 'D', 'V', 'E', 'P', 'P', 'K', 'F', 'T', 'E', 'V', 'T', 'D', 'V', 'V', 'L', 'I', 'T', 'R', 'D', 'N', 'F', 'K', 'E', 'E', 'L', 'E', 'K', 'K', 'G', 'L', 'G', 'G', 'K'], ['A', 'P', 'A', 'F', 'S', 'V', 'S', 'P', 'A', 'S', 'G', 'A', 'S', 'D', 'G', 'Q', 'S', 'V', 'S', 'V', 'S', 'V', 'A', 'A', 'A', 'G', 'E', 'T', 'Y', 'Y', 'I', 'A', 'Q', 'C', 'A', 'P', 'V', 'G', 'G', 'Q', 'D', 'A', 'C', 'N', 'P', 'A', 'T', 'A', 'T', 'S', 'F', 'T', 'T', 'D', 'A', 'S', 'G', 'A', 'A', 'S', 'F', 'S', 'F', 'T', 'V', 'R', 'K', 'S', 'Y', 'A', 'G', 'Q', 'T', 'P', 'S', 'G', 'T', 'P', 'V', 'G', 'S', 'V', 'D', 'C', 'A', 'T', 'D', 'A', 'C', 'N', 'L', 'G', 'A', 'G', 'N', 'S', 'G', 'L', 'N', 'L', 'G', 'H', 'V', 'A', 'L', 'T', 'F', 'G'], ['G', 'F', 'P', 'I', 'P', 'D', 'P', 'Y', 'C', 'W', 'D', 'I', 'S', 'F', 'R', 'T', 'F', 'Y', 'T', 'I', 'V', 'D', 'D', 'E', 'H', 'K', 'T', 'L', 'F', 'N', 'G', 'I', 'L', 'L', 'L', 'S', 'Q', 'A', 'D', 'N', 'A', 'D', 'H', 'L', 'N', 'E', 'L', 'R', 'R', 'C', 'T', 'G', 'K', 'H', 'F', 'L', 'N', 'E', 'Q', 'Q', 'L', 'M', 'Q', 'A', 'S', 'Q', 'Y', 'A', 'G', 'Y', 'A', 'E', 'H', 'K', 'K', 'A', 'H', 'D', 'D', 'F', 'I', 'H', 'K', 'L', 'D', 'T', 'W', 'D', 'G', 'D', 'V', 'T', 'Y', 'A', 'K', 'N', 'W', 'L', 'V', 'N', 'H', 'I', 'K', 'T', 'I', 'D', 'F', 'K', 'Y', 'R', 'G', 'K', 'I'], ['R', 'D', 'F', 'T', 'P', 'P', 'T', 'V', 'K', 'I', 'L', 'Q', 'S', 'S', 'C', 'D', 'G', 'G', 'G', 'H', 'F', 'P', 'P', 'T', 'I', 'Q', 'L', 'L', 'C', 'L', 'V', 'S', 'G', 'Y', 'T', 'P', 'G', 'T', 'I', 'N', 'I', 'T', 'W', 'L', 'E', 'D', 'G', 'Q', 'V', 'M', 'D', 'V', 'D', 'L', 'S', 'T', 'A', 'S', 'T', 'T', 'Q', 'E', 'G', 'E', 'L', 'A', 'S', 'T', 'Q', 'S', 'E', 'L', 'T', 'L', 'S', 'Q', 'K', 'H', 'W', 'L', 'S', 'D', 'R', 'T', 'Y', 'T', 'C', 'Q', 'V', 'T', 'Y', 'Q', 'G', 'H', 'T', 'F', 'E', 'D', 'S', 'T', 'K', 'K', 'C', 'A', 'D', 'S', 'N', 'P', 'R', 'G', 'V', 'S', 'A', 'Y', 'L', 'S', 'R', 'P', 'S', 'P', 'F', 'D', 'L', 'F', 'I', 'R', 'K', 'S', 'P', 'T', 'I', 'T', 'C', 'L', 'V', 'V', 'D', 'L', 'A', 'P', 'S', 'K', 'G', 'T', 'V', 'N', 'L', 'T', 'W', 'S', 'R', 'A', 'S', 'G', 'K', 'P', 'V', 'N', 'H', 'S', 'T', 'R', 'K', 'E', 'E', 'K', 'Q', 'R', 'N', 'G', 'T', 'L', 'T', 'V', 'T', 'S', 'T', 'L', 'P', 'V', 'G', 'T', 'R', 'D', 'W', 'I', 'E', 'G', 'E', 'T', 'Y', 'Q', 'C', 'R', 'V', 'T', 'H', 'P', 'H', 'L', 'P', 'R', 'A', 'L', 'M', 'R', 'S', 'T', 'T', 'K', 'T', 'S', 'G', 'P', 'R', 'A', 'A', 'P', 'E', 'V', 'Y', 'A', 'F', 'A', 'T', 'P', 'E', 'W', 'P', 'G', 'S', 'R', 'D', 'K', 'R', 'T', 'L', 'A', 'C', 'L', 'I', 'Q', 'N', 'F', 'M', 'P', 'E', 'D', 'I', 'S', 'V', 'Q', 'W', 'L', 'H', 'N', 'E', 'V', 'Q', 'L', 'P', 'D', 'A', 'R', 'H', 'S', 'T', 'T', 'Q', 'P', 'R', 'K', 'T', 'K', 'G', 'S', 'G', 'F', 'F', 'V', 'F', 'S', 'R', 'L', 'E', 'V', 'T', 'R', 'A', 'E', 'W', 'E', 'Q', 'K', 'D', 'E', 'F', 'I', 'C', 'R', 'A', 'V', 'H', 'E', 'A', 'A', 'S', 'P', 'S', 'Q', 'T', 'V', 'Q', 'R', 'A', 'V', 'S', 'V', 'N', 'P', 'G', 'K'], ['R', 'I', 'C', 'F', 'N', 'Q', 'H', 'S', 'S', 'Q', 'P', 'Q', 'T', 'T', 'K', 'T', 'C', 'S', 'P', 'G', 'E', 'S', 'S', 'C', 'Y', 'H', 'K', 'Q', 'W', 'S', 'D', 'F', 'R', 'G', 'T', 'I', 'I', 'E', 'R', 'G', 'C', 'G', 'C', 'P', 'T', 'V', 'K', 'P', 'G', 'I', 'K', 'L', 'S', 'C', 'C', 'E', 'S', 'E', 'V', 'C', 'N', 'N'], ['I', 'P', 'E', 'Y', 'V', 'D', 'W', 'R', 'Q', 'K', 'G', 'A', 'V', 'T', 'P', 'V', 'K', 'N', 'Q', 'G', 'S', 'C', 'G', 'S', 'C', 'W', 'A', 'F', 'S', 'A', 'V', 'V', 'T', 'I', 'E', 'G', 'I', 'I', 'K', 'I', 'R', 'T', 'G', 'N', 'L', 'N', 'Q', 'Y', 'S', 'E', 'Q', 'E', 'L', 'L', 'D', 'C', 'D', 'R', 'R', 'S', 'Y', 'G', 'C', 'N', 'G', 'G', 'Y', 'P', 'W', 'S', 'A', 'L', 'Q', 'L', 'V', 'A', 'Q', 'Y', 'G', 'I', 'H', 'Y', 'R', 'N', 'T', 'Y', 'P', 'Y', 'E', 'G', 'V', 'Q', 'R', 'Y', 'C', 'R', 'S', 'R', 'E', 'K', 'G', 'P', 'Y', 'A', 'A', 'K', 'T', 'D', 'G', 'V', 'R', 'Q', 'V', 'Q', 'P', 'Y', 'N', 'Q', 'G', 'A', 'L', 'L', 'Y', 'S', 'I', 'A', 'N', 'Q', 'P', 'V', 'S', 'V', 'V', 'L', 'Q', 'A', 'A', 'G', 'K', 'D', 'F', 'Q', 'L', 'Y', 'R', 'G', 'G', 'I', 'F', 'V', 'G', 'P', 'C', 'G', 'N', 'K', 'V', 'D', 'H', 'A', 'V', 'A', 'A', 'V', 'G', 'Y', 'G', 'P', 'N', 'Y', 'I', 'L', 'I', 'K', 'N', 'S', 'W', 'G', 'T', 'G', 'W', 'G', 'E', 'N', 'G', 'Y', 'I', 'R', 'I', 'K', 'R', 'G', 'T', 'G', 'N', 'S', 'Y', 'G', 'V', 'C', 'G', 'L', 'Y', 'T', 'S', 'S', 'F', 'Y', 'P', 'V', 'K', 'N'], ['T', 'Y', 'T', 'T', 'R', 'Q', 'I', 'G', 'A', 'K', 'N', 'T', 'L', 'E', 'Y', 'K', 'V', 'Y', 'I', 'E', 'K', 'D', 'G', 'K', 'P', 'V', 'S', 'A', 'F', 'H', 'D', 'I', 'P', 'L', 'Y', 'A', 'D', 'K', 'E', 'D', 'N', 'I', 'F', 'N', 'M', 'V', 'V', 'E', 'I', 'P', 'R', 'W', 'T', 'N', 'A', 'K', 'L', 'E', 'I', 'T', 'K', 'E', 'E', 'T', 'L', 'N', 'P', 'I', 'I', 'Q', 'N', 'T', 'K', 'G', 'K', 'L', 'R', 'F', 'V', 'R', 'N', 'C', 'F', 'P', 'H', 'H', 'G', 'Y', 'I', 'H', 'N', 'Y', 'G', 'A', 'F', 'P', 'Q', 'T', 'W', 'E', 'D', 'P', 'N', 'V', 'S', 'H', 'P', 'E', 'T', 'K', 'A', 'V', 'G', 'D', 'N', 'N', 'P', 'I', 'D', 'V', 'L', 'Q', 'I', 'G', 'E', 'T', 'I', 'A', 'Y', 'T', 'G', 'Q', 'V', 'K', 'E', 'V', 'K', 'A', 'L', 'G', 'I', 'M', 'A', 'L', 'L', 'D', 'E', 'G', 'E', 'T', 'D', 'W', 'K', 'V', 'I', 'A', 'I', 'D', 'I', 'N', 'D', 'P', 'L', 'A', 'P', 'K', 'L', 'N', 'D', 'I', 'E', 'D', 'V', 'E', 'K', 'Y', 'F', 'P', 'G', 'L', 'L', 'R', 'A', 'T', 'D', 'E', 'W', 'F', 'R', 'I', 'Y', 'K', 'I', 'P', 'D', 'G', 'K', 'P', 'E', 'N', 'Q', 'F', 'A', 'F', 'S', 'G', 'E', 'A', 'K', 'N', 'K', 'K', 'Y', 'A', 'L', 'D', 'I', 'I', 'K', 'E', 'T', 'H', 'N', 'S', 'W', 'K', 'Q', 'L', 'I', 'A', 'G', 'K', 'S', 'S', 'D', 'S', 'K', 'G', 'I', 'D', 'L', 'T', 'N', 'V', 'T', 'L', 'P', 'D', 'T', 'P', 'T', 'Y', 'S', 'K', 'A', 'A', 'S', 'D', 'A', 'I', 'P', 'P', 'A', 'S', 'P', 'K', 'A', 'D', 'A', 'P', 'I', 'D', 'K', 'S', 'I', 'D', 'K', 'W', 'F', 'F', 'I'], ['L', 'P', 'S', 'Y', 'V', 'D', 'W', 'R', 'S', 'A', 'G', 'A', 'V', 'V', 'D', 'I', 'K', 'S', 'Q', 'G', 'E', 'C', 'G', 'G', 'C', 'W', 'A', 'F', 'S', 'A', 'I', 'A', 'T', 'V', 'E', 'G', 'I', 'N', 'K', 'I', 'T', 'S', 'G', 'S', 'L', 'I', 'S', 'L', 'S', 'E', 'Q', 'E', 'L', 'I', 'D', 'C', 'G', 'R', 'T', 'Q', 'N', 'T', 'R', 'G', 'C', 'D', 'G', 'G', 'Y', 'I', 'T', 'D', 'G', 'F', 'Q', 'F', 'I', 'I', 'N', 'D', 'G', 'G', 'I', 'N', 'T', 'E', 'E', 'N', 'Y', 'P', 'Y', 'T', 'A', 'Q', 'D', 'G', 'D', 'C', 'D', 'V', 'A', 'L', 'Q', 'D', 'Q', 'K', 'Y', 'V', 'T', 'I', 'D', 'T', 'Y', 'E', 'N', 'V', 'P', 'Y', 'N', 'N', 'E', 'W', 'A', 'L', 'Q', 'T', 'A', 'V', 'T', 'Y', 'Q', 'P', 'V', 'S', 'V', 'A', 'L', 'D', 'A', 'A', 'G', 'D', 'A', 'F', 'K', 'Q', 'Y', 'A', 'S', 'G', 'I', 'F', 'T', 'G', 'P', 'C', 'G', 'T', 'A', 'V', 'D', 'H', 'A', 'I', 'V', 'I', 'V', 'G', 'Y', 'G', 'T', 'E', 'G', 'G', 'V', 'D', 'Y', 'W', 'I', 'V', 'K', 'N', 'S', 'W', 'D', 'T', 'T', 'W', 'G', 'E', 'E', 'G', 'Y', 'M', 'R', 'I', 'L', 'R', 'N', 'V', 'G', 'G', 'A', 'G', 'T', 'C', 'G', 'I', 'A', 'T', 'M', 'P', 'S', 'Y', 'P', 'V', 'K', 'Y'], ['A', 'N', 'I', 'V', 'G', 'G', 'I', 'E', 'Y', 'S', 'I', 'N', 'N', 'A', 'S', 'L', 'C', 'S', 'V', 'G', 'F', 'S', 'V', 'T', 'R', 'G', 'A', 'T', 'K', 'G', 'F', 'V', 'T', 'A', 'G', 'H', 'C', 'G', 'T', 'V', 'N', 'A', 'T', 'A', 'R', 'I', 'G', 'G', 'A', 'V', 'V', 'G', 'T', 'F', 'A', 'A', 'R', 'V', 'F', 'P', 'G', 'N', 'D', 'R', 'A', 'W', 'V', 'S', 'L', 'T', 'S', 'A', 'Q', 'T', 'L', 'L', 'P', 'R', 'V', 'A', 'N', 'G', 'S', 'S', 'F', 'V', 'T', 'V', 'R', 'G', 'S', 'T', 'E', 'A', 'A', 'V', 'G', 'A', 'A', 'V', 'C', 'R', 'S', 'G', 'R', 'T', 'T', 'G', 'Y', 'Q', 'C', 'G', 'T', 'I', 'T', 'A', 'K', 'N', 'V', 'T', 'A', 'N', 'Y', 'A', 'E', 'G', 'A', 'V', 'R', 'G', 'L', 'T', 'Q', 'G', 'N', 'A', 'C', 'M', 'G', 'R', 'G', 'D', 'S', 'G', 'G', 'S', 'W', 'I', 'T', 'S', 'A', 'G', 'Q', 'A', 'Q', 'G', 'V', 'M', 'S', 'G', 'G', 'N', 'V', 'Q', 'S', 'N', 'G', 'N', 'N', 'C', 'G', 'I', 'P', 'A', 'S', 'Q', 'R', 'S', 'S', 'L', 'F', 'E', 'R', 'L', 'Q', 'P', 'I', 'L', 'S', 'Q', 'Y', 'G', 'L', 'S', 'L', 'V', 'T', 'G'], ['A', 'P', 'K', 'A', 'P', 'A', 'D', 'G', 'L', 'K', 'M', 'D', 'K', 'T', 'K', 'Q', 'P', 'V', 'V', 'F', 'N', 'H', 'S', 'T', 'H', 'K', 'A', 'V', 'K', 'C', 'G', 'D', 'C', 'H', 'H', 'P', 'V', 'N', 'G', 'K', 'E', 'N', 'Y', 'Q', 'K', 'C', 'A', 'T', 'A', 'G', 'C', 'H', 'D', 'N', 'M', 'D', 'K', 'K', 'D', 'K', 'S', 'A', 'K', 'G', 'Y', 'Y', 'H', 'A', 'M', 'H', 'D', 'K', 'G', 'T', 'K', 'F', 'K', 'S', 'C', 'V', 'G', 'C', 'H', 'L', 'E', 'T', 'A', 'G', 'A', 'D', 'A', 'A', 'K', 'K', 'K', 'E', 'L', 'T', 'G', 'C', 'K', 'G', 'S', 'K', 'C', 'H', 'S'], ['V', 'A', 'S', 'Y', 'D', 'Y', 'L', 'V', 'I', 'G', 'G', 'G', 'S', 'G', 'G', 'L', 'A', 'S', 'A', 'R', 'R', 'A', 'A', 'E', 'L', 'G', 'A', 'R', 'A', 'A', 'V', 'V', 'E', 'S', 'H', 'K', 'L', 'G', 'G', 'T', 'C', 'V', 'N', 'V', 'G', 'C', 'V', 'P', 'K', 'K', 'V', 'M', 'W', 'N', 'T', 'A', 'V', 'H', 'S', 'E', 'F', 'M', 'H', 'D', 'H', 'A', 'V', 'Y', 'G', 'F', 'P', 'S', 'C', 'E', 'G', 'F', 'F', 'N', 'W', 'R', 'V', 'I', 'K', 'E', 'K', 'R', 'D', 'A', 'Y', 'V', 'S', 'R', 'L', 'N', 'A', 'I', 'Y', 'Q', 'N', 'N', 'L', 'T', 'K', 'S', 'H', 'I', 'E', 'I', 'I', 'R', 'G', 'H', 'A', 'A', 'F', 'T', 'S', 'D', 'P', 'K', 'P', 'T', 'I', 'E', 'V', 'S', 'G', 'K', 'K', 'Y', 'T', 'A', 'P', 'H', 'I', 'L', 'I', 'A', 'T', 'G', 'G', 'M', 'P', 'S', 'T', 'P', 'H', 'E', 'S', 'Q', 'I', 'P', 'G', 'A', 'S', 'L', 'G', 'I', 'T', 'S', 'D', 'G', 'F', 'F', 'Q', 'L', 'E', 'E', 'L', 'P', 'G', 'R', 'S', 'V', 'I', 'V', 'G', 'A', 'G', 'Y', 'I', 'A', 'V', 'E', 'M', 'A', 'G', 'I', 'L', 'S', 'A', 'L', 'G', 'S', 'K', 'T', 'S', 'L', 'M', 'I', 'R', 'H', 'D', 'N', 'V', 'L', 'R', 'S', 'F', 'D', 'S', 'M', 'I', 'S', 'T', 'N', 'C', 'T', 'E', 'E', 'L', 'E', 'N', 'A', 'G', 'V', 'E', 'V', 'L', 'K', 'F', 'S', 'Q', 'V', 'K', 'E', 'V', 'K', 'K', 'T', 'L', 'S', 'G', 'L', 'E', 'V', 'S', 'M', 'V', 'T', 'A', 'V', 'P', 'G', 'R', 'L', 'P', 'V', 'M', 'T', 'M', 'I', 'P', 'D', 'V', 'D', 'C', 'L', 'L', 'W', 'A', 'I', 'G', 'R', 'V', 'P', 'N', 'T', 'K', 'D', 'L', 'S', 'L', 'N', 'K', 'L', 'G', 'I', 'Q', 'T', 'D', 'D', 'K', 'G', 'H', 'I', 'I', 'V', 'D', 'E', 'F', 'Q', 'N', 'T', 'N', 'V', 'K', 'G', 'I', 'Y', 'A', 'V', 'G', 'D', 'V', 'C', 'G', 'K', 'A', 'L', 'L', 'T', 'P', 'V', 'A', 'I', 'A', 'A', 'G', 'R', 'K', 'L', 'A', 'H', 'R', 'L', 'F', 'E', 'Y', 'K', 'E', 'D', 'S', 'K', 'L', 'D', 'Y', 'N', 'N', 'I', 'P', 'T', 'V', 'V', 'F', 'S', 'H', 'P', 'P', 'I', 'G', 'T', 'V', 'G', 'L', 'T', 'E', 'D', 'E', 'A', 'I', 'H', 'K', 'Y', 'G', 'I', 'E', 'N', 'V', 'K', 'T', 'Y', 'S', 'T', 'S', 'F', 'T', 'P', 'M', 'Y', 'H', 'A', 'V', 'T', 'K', 'R', 'K', 'T', 'K', 'C', 'V', 'M', 'K', 'M', 'V', 'C', 'A', 'N', 'K', 'E', 'E', 'K', 'V', 'V', 'G', 'I', 'H', 'M', 'Q', 'G', 'L', 'G', 'C', 'D', 'E', 'M', 'L', 'Q', 'G', 'F', 'A', 'V', 'A', 'V', 'K', 'M', 'G', 'A', 'T', 'K', 'A', 'D', 'F', 'D', 'N', 'T', 'V', 'A', 'I', 'H', 'P', 'T', 'S', 'S', 'E', 'E', 'L', 'V', 'T', 'L', 'R'], ['P', 'I', 'V', 'D', 'T', 'G', 'S', 'V', 'A', 'P', 'L', 'S', 'A', 'A', 'E', 'K', 'T', 'K', 'I', 'R', 'S', 'A', 'W', 'A', 'P', 'V', 'Y', 'S', 'T', 'Y', 'E', 'T', 'S', 'G', 'V', 'D', 'I', 'L', 'V', 'K', 'F', 'F', 'T', 'S', 'T', 'P', 'A', 'A', 'Q', 'E', 'F', 'F', 'P', 'K', 'F', 'K', 'G', 'L', 'T', 'T', 'A', 'D', 'E', 'L', 'K', 'K', 'S', 'A', 'D', 'V', 'R', 'W', 'H', 'A', 'E', 'R', 'I', 'I', 'N', 'A', 'V', 'D', 'D', 'A', 'V', 'A', 'S', 'M', 'D', 'D', 'T', 'E', 'K', 'M', 'S', 'M', 'K', 'L', 'R', 'N', 'L', 'S', 'G', 'K', 'H', 'A', 'K', 'S', 'F', 'Q', 'V', 'D', 'P', 'E', 'Y', 'F', 'K', 'V', 'L', 'A', 'A', 'V', 'I', 'A', 'D', 'T', 'V', 'A', 'A', 'G', 'D', 'A', 'G', 'F', 'E', 'K', 'L', 'M', 'S', 'M', 'I', 'C', 'I', 'L', 'L', 'R', 'S', 'A', 'Y'], ['A', 'Q', 'S', 'V', 'P', 'Y', 'G', 'V', 'S', 'Q', 'I', 'K', 'A', 'P', 'A', 'L', 'H', 'S', 'Q', 'G', 'Y', 'T', 'G', 'S', 'N', 'V', 'K', 'V', 'A', 'V', 'I', 'D', 'S', 'G', 'I', 'D', 'S', 'S', 'H', 'P', 'D', 'L', 'K', 'V', 'A', 'G', 'G', 'A', 'S', 'M', 'V', 'P', 'S', 'E', 'T'], ['P', 'N', 'F', 'Q', 'D', 'D', 'N', 'S', 'H', 'G', 'T', 'H', 'V', 'A', 'G', 'T', 'V', 'A', 'A', 'L', 'N', 'N', 'S', 'I', 'G', 'V', 'L', 'G', 'V', 'A', 'P', 'S', 'S', 'A', 'L', 'Y', 'A', 'V', 'K', 'V', 'L', 'G', 'D', 'A', 'G', 'S', 'G', 'Q', 'Y', 'S', 'W', 'I', 'I', 'N', 'G', 'I', 'E', 'W', 'A', 'I', 'A', 'N', 'N', 'M', 'D', 'V', 'I', 'N', 'M', 'S', 'L', 'G', 'G', 'P', 'S', 'G', 'S', 'A', 'A', 'L', 'K', 'A', 'A', 'V', 'D', 'K', 'A', 'V', 'A', 'S', 'G', 'V', 'V', 'V', 'V', 'A', 'A', 'A', 'G', 'N', 'E', 'G', 'S', 'T', 'G', 'S', 'S', 'S', 'T', 'V', 'G', 'Y', 'P', 'G', 'K', 'Y', 'P', 'S', 'V', 'I', 'A', 'V', 'G', 'A', 'V', 'D', 'S', 'S', 'N', 'Q', 'R', 'A', 'S', 'F', 'S', 'S', 'V', 'G', 'P', 'E', 'L', 'D', 'V', 'M', 'A', 'P', 'G', 'V', 'S', 'I', 'Q', 'S', 'T', 'L', 'P', 'G', 'N', 'K', 'Y', 'G', 'A', 'Y', 'N', 'G', 'T', 'S', 'M', 'A', 'S', 'P', 'H', 'V', 'A', 'G', 'A', 'A', 'A', 'L', 'I', 'L', 'S', 'K', 'H', 'P', 'N', 'W', 'T', 'N', 'T', 'Q', 'V', 'R', 'S', 'S', 'L', 'Q', 'N', 'T', 'T', 'T', 'K', 'L', 'G', 'D', 'S', 'F', 'Y', 'Y', 'G', 'K', 'G', 'L', 'I', 'N', 'V', 'Q', 'A', 'A', 'A', 'Q'], ['G', 'K', 'V', 'K', 'V', 'G', 'V', 'D', 'G', 'F', 'G', 'R', 'I', 'G', 'R', 'L', 'V', 'T', 'R', 'A', 'A', 'F', 'N', 'S', 'G', 'K', 'V', 'D', 'I', 'V', 'A', 'I', 'N', 'D', 'P', 'F', 'I', 'D', 'L', 'H', 'Y', 'M', 'V', 'Y', 'M', 'F', 'Q', 'Y', 'D', 'S', 'T', 'H', 'G', 'K', 'F', 'H', 'G', 'T', 'V', 'K', 'A', 'E', 'D', 'G', 'K', 'L', 'V', 'I', 'D', 'G', 'K', 'A', 'I', 'T', 'I', 'F', 'Q', 'E', 'R', 'D', 'P', 'E', 'N', 'I', 'K', 'W', 'G', 'D', 'A', 'G', 'T', 'A', 'Y', 'V', 'V', 'E', 'S', 'T', 'G', 'V', 'F', 'T', 'T', 'M', 'E', 'K', 'A', 'G', 'A', 'H', 'L', 'K', 'G', 'G', 'A', 'K', 'R', 'I', 'V', 'I', 'S', 'A', 'P', 'S', 'A', 'D', 'A', 'P', 'M', 'F', 'V', 'M', 'G', 'V', 'N', 'H', 'F', 'K', 'Y', 'A', 'N', 'S', 'L', 'K', 'I', 'I', 'S', 'N', 'A', 'S', 'C', 'T', 'T', 'N', 'C', 'L', 'A', 'P', 'L', 'A', 'K', 'V', 'I', 'H', 'D', 'H', 'F', 'G', 'I', 'V', 'E', 'G', 'L', 'M', 'T', 'T', 'V', 'H', 'A', 'I', 'T', 'A', 'T', 'Q', 'K', 'T', 'V', 'D', 'S', 'P', 'S', 'G', 'K', 'L', 'W', 'R', 'G', 'G', 'R', 'G', 'A', 'A', 'Q', 'N', 'L', 'I', 'P', 'A', 'S', 'T', 'G', 'A', 'A', 'K', 'A', 'V', 'G', 'K', 'V', 'I', 'P', 'E', 'L', 'D', 'G', 'K', 'L', 'T', 'G', 'M', 'A', 'F', 'R', 'V', 'P', 'T', 'A', 'N', 'V', 'S', 'V', 'L', 'D', 'L', 'T', 'C', 'R', 'L', 'E', 'K', 'P', 'A', 'K', 'Y', 'D', 'D', 'I', 'K', 'K', 'V', 'V', 'K', 'E', 'A', 'S', 'E', 'G', 'P', 'L', 'K', 'G', 'I', 'L', 'G', 'Y', 'T', 'E', 'D', 'E', 'V', 'V', 'S', 'D', 'D', 'F', 'N', 'G', 'S', 'N', 'H', 'S', 'S', 'I', 'F', 'D', 'A', 'G', 'A', 'G', 'I', 'E', 'L', 'N', 'D', 'T', 'F', 'V', 'K', 'L', 'V', 'S', 'W', 'Y', 'D', 'N', 'E', 'F', 'G', 'Y', 'S', 'E', 'R', 'V', 'V', 'D', 'L', 'M', 'A', 'H', 'M', 'A', 'S', 'K', 'E'], ['H', 'P', 'T', 'F', 'N', 'K', 'I', 'T', 'P', 'N', 'L', 'A', 'E', 'F', 'A', 'F', 'S', 'L', 'Y', 'R', 'Q', 'L', 'A', 'H', 'Q', 'S', 'N', 'S', 'T', 'N', 'I', 'F', 'F', 'S', 'P', 'V', 'S', 'I', 'A', 'T', 'A', 'F', 'A', 'M', 'L', 'S', 'L', 'G', 'T', 'K', 'A', 'D', 'T', 'H', 'D', 'E', 'I', 'L', 'E', 'G', 'L', 'N', 'F', 'N', 'L', 'T', 'E', 'I', 'P', 'E', 'A', 'Q', 'I', 'H', 'E', 'G', 'F', 'Q', 'E', 'L', 'L', 'R', 'T', 'L', 'N', 'Q', 'P', 'D', 'S', 'Q', 'L', 'Q', 'L', 'T', 'T', 'G', 'N', 'G', 'L', 'F', 'L', 'S', 'E', 'G', 'L', 'K', 'L', 'V', 'D', 'K', 'F', 'L', 'E', 'D', 'V', 'K', 'K', 'L', 'Y', 'H', 'S', 'E', 'A', 'F', 'T', 'V', 'N', 'F', 'G', 'D', 'T', 'E', 'E', 'A', 'K', 'K', 'Q', 'I', 'N', 'D', 'Y', 'V', 'E', 'K', 'G', 'T', 'Q', 'G', 'K', 'I', 'V', 'D', 'L', 'V', 'K', 'E', 'L', 'D', 'R', 'D', 'T', 'V', 'F', 'A', 'L', 'V', 'N', 'Y', 'I', 'F', 'F', 'K', 'G', 'K', 'W', 'E', 'R', 'P', 'F', 'E', 'V', 'K', 'D', 'T', 'E', 'E', 'E', 'D', 'F', 'H', 'V', 'D', 'Q', 'V', 'T', 'T', 'V', 'K', 'V', 'P', 'M', 'M', 'K', 'R', 'L', 'G', 'M', 'F', 'N', 'I', 'Q', 'H', 'C', 'K', 'K', 'L', 'S', 'S', 'W', 'V', 'L', 'L', 'M', 'K', 'Y', 'L', 'G', 'N', 'A', 'T', 'A', 'I', 'F', 'F', 'L', 'P', 'D', 'E', 'G', 'K', 'L', 'Q', 'H', 'L', 'E', 'N', 'E', 'L', 'T', 'H', 'D', 'I', 'I', 'T', 'K', 'F', 'L', 'E', 'N', 'E', 'D', 'R', 'R', 'S', 'A', 'S', 'L', 'H', 'L', 'P', 'K', 'L', 'S', 'I', 'T', 'G', 'T', 'Y', 'D', 'L', 'K', 'S', 'V', 'L', 'G', 'Q', 'L', 'G', 'I', 'T', 'K', 'V', 'F', 'S', 'N', 'G', 'A', 'D', 'L', 'S', 'G', 'V', 'T', 'E', 'E', 'A', 'P', 'L', 'K', 'L', 'S', 'K', 'A', 'V', 'H', 'K', 'A', 'V', 'L', 'T', 'I', 'D', 'E', 'K', 'G', 'T', 'E', 'A', 'A', 'G', 'A', 'M', 'F', 'L', 'E', 'A', 'I', 'P', 'M'], ['S', 'I', 'P', 'P', 'E', 'V', 'K', 'F', 'N', 'K', 'P', 'F', 'V', 'F', 'L', 'M', 'I', 'E', 'Q', 'N', 'T', 'K', 'S', 'P', 'L', 'F', 'M', 'G', 'K', 'V', 'V', 'N', 'P', 'T', 'Q']]\n",
            "[['_', '_', '_', '_', '_', '_', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', 'e', 'e', '_', '_', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', '_', '_', '_', 'e', 'e', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', '_', '_', '_', '_', '_', 'e', 'e', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_'], ['_', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_'], ['_', '_', '_', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_'], ['_', 'e', 'e', 'e', 'e', '_', '_', 'e', 'e', 'e', 'e', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', 'e', 'e', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', '_', '_', 'e', 'e', '_', '_', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', 'e', 'e', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', '_', '_', '_', '_', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', 'e', 'e', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_', '_', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_'], ['_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', '_', '_', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'e', 'e', '_', '_', '_', 'e', 'e', '_', '_', '_', '_', 'e', 'e', '_', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_'], ['_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', 'e', 'e', 'e', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', 'e', 'e', 'e', 'e', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', 'h', 'h', 'h', 'h', 'h', 'h', 'h', 'h', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_'], ['_', '_', '_', '_', 'e', 'e', 'e', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', '_', '_', '_', '_', '_']]\n",
            "                                           acideAmine  \\\n",
            "0   [3, 11, 9, 8, 9, 5, 4, 9, 17, 8, 13, 12, 3, 3,...   \n",
            "1   [0, 12, 0, 4, 15, 17, 15, 12, 0, 15, 5, 0, 15,...   \n",
            "2   [5, 4, 12, 7, 12, 2, 12, 19, 1, 18, 2, 7, 15, ...   \n",
            "3   [14, 2, 4, 16, 12, 12, 16, 17, 8, 7, 9, 13, 15...   \n",
            "4   [14, 7, 1, 4, 11, 13, 6, 15, 15, 13, 12, 13, 1...   \n",
            "5   [7, 12, 3, 19, 17, 2, 18, 14, 13, 8, 5, 0, 17,...   \n",
            "6   [16, 19, 16, 16, 14, 13, 7, 5, 0, 8, 11, 16, 9...   \n",
            "7   [9, 12, 15, 19, 17, 2, 18, 14, 15, 0, 5, 0, 17...   \n",
            "8   [0, 11, 7, 17, 5, 5, 7, 3, 19, 15, 7, 11, 11, ...   \n",
            "9   [0, 12, 8, 0, 12, 0, 2, 5, 9, 8, 10, 2, 8, 16,...   \n",
            "10  [17, 0, 15, 19, 2, 19, 9, 17, 7, 5, 5, 5, 15, ...   \n",
            "11  [12, 7, 17, 2, 16, 5, 15, 17, 0, 12, 9, 15, 0,...   \n",
            "12  [0, 13, 15, 17, 12, 19, 5, 17, 15, 13, 7, 8, 0...   \n",
            "13  [12, 11, 4, 13, 2, 2, 11, 15, 6, 5, 16, 6, 17,...   \n",
            "14  [5, 8, 17, 8, 17, 5, 17, 2, 5, 4, 5, 14, 7, 5,...   \n",
            "15  [6, 12, 16, 4, 11, 8, 7, 16, 12, 11, 9, 0, 3, ...   \n",
            "16  [15, 7, 12, 12, 3, 17, 8, 4, 11, 8, 12, 4, 17,...   \n",
            "\n",
            "                                                label  \n",
            "0   [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...  \n",
            "1   [0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "2   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "3   [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...  \n",
            "4   [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...  \n",
            "5   [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "6   [0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "7   [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "8   [0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, ...  \n",
            "9   [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, ...  \n",
            "10  [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, ...  \n",
            "11  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, ...  \n",
            "12  [0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, ...  \n",
            "13  [0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, ...  \n",
            "14  [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, ...  \n",
            "15  [0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  \n",
            "16  [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, ...  \n",
            "torch.Size([111, 498, 21])\n",
            "torch.Size([111, 498, 4])\n",
            "tensor([[[1., 0., 0., 0.],\n",
            "         [1., 0., 0., 0.],\n",
            "         [1., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0., 1.],\n",
            "         [0., 0., 0., 1.],\n",
            "         [0., 0., 0., 1.]],\n",
            "\n",
            "        [[1., 0., 0., 0.],\n",
            "         [1., 0., 0., 0.],\n",
            "         [0., 1., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0., 1.],\n",
            "         [0., 0., 0., 1.],\n",
            "         [0., 0., 0., 1.]],\n",
            "\n",
            "        [[1., 0., 0., 0.],\n",
            "         [1., 0., 0., 0.],\n",
            "         [1., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0., 1.],\n",
            "         [0., 0., 0., 1.],\n",
            "         [0., 0., 0., 1.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1., 0., 0., 0.],\n",
            "         [1., 0., 0., 0.],\n",
            "         [1., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0., 1.],\n",
            "         [0., 0., 0., 1.],\n",
            "         [0., 0., 0., 1.]],\n",
            "\n",
            "        [[1., 0., 0., 0.],\n",
            "         [1., 0., 0., 0.],\n",
            "         [1., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0., 1.],\n",
            "         [0., 0., 0., 1.],\n",
            "         [0., 0., 0., 1.]],\n",
            "\n",
            "        [[1., 0., 0., 0.],\n",
            "         [1., 0., 0., 0.],\n",
            "         [1., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0., 1.],\n",
            "         [0., 0., 0., 1.],\n",
            "         [0., 0., 0., 1.]]], device='cuda:0')\n",
            "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
            "\n",
            "        [[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 1.]]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model **\n",
        "Le choix du model est un Gated Recurrent Unit (GRU) bidirectionnelle car il a donnée de meilleur résultat que le unidirectionnelle\n",
        "\n"
      ],
      "metadata": {
        "id": "s1uh6Uce-JIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Hyper-parameters \n",
        "num_epochs = 2\n",
        "batch_size = 64\n",
        "learning_rate = 0.1\n",
        "input_size = 21\n",
        "hidden_size = 16\n",
        "num_layers = 2\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.num_layers = 2\n",
        "        self.hidden_size = 16\n",
        "        self.gru = nn.GRU(input_size, 16, num_layers*2, batch_first=True,bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_size*2, 4)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        \n",
        "    def forward(self, x,h0):\n",
        "          \n",
        "          out, o = self.gru(x, h0)  \n",
        "          \n",
        "          a = nn.Sequential(\n",
        "            self.relu,\n",
        "            self.fc)\n",
        "          \n",
        "          out = a(out)\n",
        "          # out: (n, 10)\n",
        "          return out , o\n",
        "          \n",
        "    def init_hidden(self, batch_size):\n",
        "        \n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        \n",
        "\n",
        "\n",
        "model =  NeuralNetwork(input_size, hidden_size, num_layers)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J-hdk7Nz5rl",
        "outputId": "5c65a220-5b56-45dc-86b5-ab77336255cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetwork(\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (gru): GRU(21, 16, num_layers=4, batch_first=True, bidirectional=True)\n",
              "  (fc): Linear(in_features=32, out_features=4, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HsFpA5VrmsW",
        "outputId": "ab0b8fd8-bdd9-4528-bb05-f4f57edf98a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cellule que j'utilise pas \n",
        "dataset = Dataset(x_train,y_train)\n",
        "train_loader = DataLoader(dataset=dataset,batch_size=64, shuffle=True)\n",
        "datasettest = Dataset(x_test,y_test)\n",
        "test_loader = DataLoader(dataset=datasettest,batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "J5I0YoRQ0CEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "dZoP9KGuLnVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader.dataset):\n",
        "        \n",
        "        h = model.init_hidden(len(X))\n",
        "        p , h =model(X,h)\n",
        "        loss = loss_fn(p, y)\n",
        "        \n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  \")"
      ],
      "metadata": {
        "id": "5dHoMy7m0lT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "G6uWOwxqLpRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "j'ai créer la fonction test qui run le modèle et vérifier l'accuracy qui est tout simplement un compteur  qui compte chaque prédiction correspondant à l'étiquette."
      ],
      "metadata": {
        "id": "d15jPxn1KbgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    print(num_batches)\n",
        "    test_loss, correct = 0, 0\n",
        "    size_no_padding=0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            \n",
        "            h = model.init_hidden(len(X))\n",
        "            pred , h =model(X,h)\n",
        "\n",
        "\n",
        "            \n",
        "            #Etape pour enlevé le padding qui faussé les résulat et me donner des accuracy \n",
        "            #illogiquement élevé\n",
        "            mask = (torch.ones(y.shape[0], y.shape[1])*(y.shape[2]-1)).to(device) != y.argmax(dim=2)\n",
        "            pred_no_padding = torch.masked_select(pred.argmax(dim=2), mask)\n",
        "            y_no_padding = torch.masked_select(y.argmax(dim=2), mask)\n",
        "\n",
        "            correct += (pred_no_padding == y_no_padding).type(torch.float).sum().item()   # compte du nombre de prédiction correctes\n",
        "            size_no_padding += len(pred_no_padding)\n",
        "            \n",
        "            loss = loss_fn(pred, y)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "\n",
        "    #''' test_loss /= num_batches\n",
        "    correct /= size_no_padding \n",
        "    \n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "uNEiS7wu1gwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.1,momentum=0.9)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "for t in range(20):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(loadtrain, model, loss_fn, optimizer)\n",
        "    test_loop(loadtest, model,  loss_fn)\n",
        "    \n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHpSZ1rR4A5f",
        "outputId": "d3b7a976-6ecb-4aee-e8ac-811d17c673c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.252125  \n",
            "loss: 0.046541  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 54.3%, Avg loss: 0.078148 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.072395  \n",
            "loss: 0.045737  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 54.5%, Avg loss: 0.076390 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.070839  \n",
            "loss: 0.042950  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 54.8%, Avg loss: 0.072635 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.060507  \n",
            "loss: 0.040405  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 56.0%, Avg loss: 0.071448 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.054146  \n",
            "loss: 0.038544  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 58.2%, Avg loss: 0.068757 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.054742  \n",
            "loss: 0.036766  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 59.8%, Avg loss: 0.066876 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.059151  \n",
            "loss: 0.035820  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 62.1%, Avg loss: 0.064601 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.062716  \n",
            "loss: 0.034327  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 62.6%, Avg loss: 0.063760 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.065110  \n",
            "loss: 0.033068  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 62.6%, Avg loss: 0.063552 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.066258  \n",
            "loss: 0.032073  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 62.6%, Avg loss: 0.063473 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.066672  \n",
            "loss: 0.031369  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 62.6%, Avg loss: 0.063400 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.066956  \n",
            "loss: 0.030592  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 62.4%, Avg loss: 0.063040 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.067031  \n",
            "loss: 0.029798  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 62.5%, Avg loss: 0.062804 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.067131  \n",
            "loss: 0.028988  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 62.4%, Avg loss: 0.062582 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.067151  \n",
            "loss: 0.028179  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 62.8%, Avg loss: 0.062145 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.066661  \n",
            "loss: 0.027267  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 63.1%, Avg loss: 0.061869 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.066451  \n",
            "loss: 0.026336  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 63.4%, Avg loss: 0.061575 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.065872  \n",
            "loss: 0.025325  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 63.4%, Avg loss: 0.061109 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.065234  \n",
            "loss: 0.024551  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 63.3%, Avg loss: 0.060663 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.065098  \n",
            "loss: 0.023733  \n",
            "2\n",
            "Test Error: \n",
            " Accuracy: 63.3%, Avg loss: 0.060319 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}